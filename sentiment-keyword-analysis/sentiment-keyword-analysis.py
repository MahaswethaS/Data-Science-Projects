# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vG5nKmwlKENwR9aJrqbiu3jdpdhJln0_

# Preprocessing
"""

!pip install opendatasets

import opendatasets as od

# URL of the Flipkart dataset on Kaggle
od.download("https://www.kaggle.com/datasets/niraliivaghani/flipkart-dataset")

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/flipkart-dataset/Dataset.csv',encoding='ISO-8859-1')

df.head()

# Check for any non-numeric values in the Price column
df['Price'].unique()  # This will help identify strange values like empty strings

# Remove any unwanted characters from the Price column and convert to numeric, forcing errors to NaN
df['Price'] = pd.to_numeric(df['Price'].replace({'[^0-9.]': ''}, regex=True), errors='coerce')

# Check if there are any NaN values after the conversion
df['Price'].isna().sum()

# Optionally, remove rows with NaN prices
df = df.dropna(subset=['Price'])

# Display the cleaned dataframe
df.head()

# Descriptive statistics for numerical columns
df.describe()

"""

```
# This is formatted as code
```

# Category"""

import pandas as pd
import re

# Define the category keywords
categories_keywords = {
    'Electronics': list(set([
        'dslr camera', 'remote', 'tata sky', 'airtel', 'google nest', 'bulb', 'lamp', 'lantern',
        'tv', 'charger', 'adapter', 'mobile phone', 'power bank', 'car amplifier',
        'wireless router', 'wifi', 'ink catridge', 'ipad', 'tablet', 'bluetooth home theatre',
        'bluetooth', 'jbl', 'boat', 'cable', 'hdmi', 'dish set', 'streaming device', 'asus',
        'keyboard', 'mouse', 'hp', 'i3', 'i5', 'i7', 'i9', 'spinbot', 'cd/dvd', 'wi-fi',
        'router', 'speaker', 'usb-c', 'inverter', 'ups', 'mobile', 'audio', 'lenovo', 'poco',
        'tds meter', 'android','printers'
    ])),
    'Appliances': [
        'dish washing gloves', 'kitchen platform', 'shower laser light', 'heater rod',
        'water purifier', 'garment steamer', 'refrigerator', 'washing machine', 'wet grinder',
        'air cooler', 'water geyser', 'vacuum cleaner', 'auto clean chimney', 'chimney', 'fan',
        'sewing machine', 'iron', 'water','dishwasher','washing machine','top load'
    ],
    'Sports and Apparel': [
        'sports cap', 'regular cap', 'plastic bat', 'abdomen support', 'cricket bat',
        'cricket stumps', 'stumps', 'tennis ball', 'football', 'wrist support',
        'wrist band', 'knee support', 'palm support', 'elbow support', 'ankle support',
        'cycling', 'cycle', 'safety shoes', 'skating shoe', 'skateboard', 'roller skates',
        'swimming kits', 'ear plugs', 'swimming', 'solid beanie', 'beanie', 'willow cricket bat',
        'mrf', 'cricket', 'exercise', 'thumb & finger sleeve', 'finger sleeves', 'leather','tummy trimmer'
        ,'silicone gel'
    ],
    'Beauty and Skin Care': [
        'face care kit', 'hair and care', 'skincare', 'moisturizer', 'serum', 'cream',
        'shampoo', 'beauty soap', 'face scrub', 'body scrub', 'hair & care', 'perfumes',
        'perfume', 'mask', 'parfum', 'envy', 'denver','kajal'
    ],
    'Cleaning Supplies': [
        'scotch-brite', 'hand gloves', 'dry glove set', 'spotzero', 'wash scrubber',
        'shower cleaner', 'dry broom', 'detergent', 'fabric conditioner', 'spin mop', 'broom',
        'mop', 'wet glove', 'dry glove', 'wet and dry glove', 'brush feather', 'cleaning cloth',
        'floor wiper', 'dustbin', 'home pest control', 'cleaning duster', 'duster', 'tuffy brush'
        ,'brush scrubber','bosch','kadai cleaning'
    ],
    'Furniture and Home': [
        'comforter', 'cushion pack', 'cotton runner', 'polyester runner', 'carpet', 'wallpaper',
        'chair', 'table', 'rack', 'shelf', 'stand', 'bookcase', 'computer desk', 'almirah',
        'cupboard', 'bedsheet', 'blanket', 'door mat', 'floor mat', 'mat', 'wardrobe',
        'pocket key holder', 'rosewood', 'rug', 'paint', 'emulsion', 'led', 'lights', 'wall',
        'furniture', 'milton', 'anti cut','key holder','sofa','wishpool','nimwash','sambrani', 'incense holder', 'mix cones'
    ],
    'Party Supplies': [
        'happy birthday combo', 'balloon', 'decorations', 'smart kits'
    ],
    'Stationery and Crafts': [
        'pencil', 'art', 'doms', 'colourup', 'ball pen', 'craft items', 'clock', 'crafts',
        'stationery', 'tape', 'adhesive', 'book', 'notes','writing pad','penstand','organizer box'
    ],
    'Toys': [
        'truck cars', 'helicopter', 'radio control race car', 'robot', 'remote control car',
        'modern car', 'flute', 'teddy', 'car','dancing cactus','temperado talking'
    ],
    'Baking and Cooking': [
        'dinner set', 'cocoa powder', 'dry yeast', 'liquid food color', 'juicer',
        'grinder', 'juicer grinder', 'food processor', 'electric rice cooker', 'cooker',
        'kettle', 'single pot', 'oven', 'mixer grinder', 'silverware', 'stainless'
    ],
    'Health and Wellness': [
        'zandu balm', 'balm', 'multivitamin', 'nutrition', 'dettol','diaper','mamypoko'
    ]
}

# Categorization function using regex word boundaries
def categorize_product(name):
    if pd.isna(name):
        return 'Miscellaneous'
    name = name.lower()
    for category, keywords in categories_keywords.items():
        for kw in keywords:
            # Escape keyword for regex and use word boundaries
            pattern = r'\b' + re.escape(kw.lower()) + r'\b'
            if re.search(pattern, name):
                return category
    return 'Miscellaneous'

# Load dataset
df = pd.read_csv('/content/flipkart-dataset/Dataset.csv', encoding='latin1')

# Apply categorization
df['category'] = df['Product_name'].apply(categorize_product)

# Save updated dataset
df.to_csv('/content/flipkart_with_categories.csv', index=False)

print("Product categorization completed and saved.")

print(df['category'].value_counts())

"""# Sentiment Analysis"""

!pip install vaderSentiment

!pip install scikit-learn

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import pandas as pd
import matplotlib.pyplot as plt

# Load your dataset
df = pd.read_csv('/content/flipkart_with_categories.csv')

# Initialize VADER
analyzer = SentimentIntensityAnalyzer()

# Functions for sentiment score and category
def get_sentiment_score(text):
    return analyzer.polarity_scores(str(text))['compound']

def categorize_sentiment(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Apply sentiment analysis
df['sentiment_score'] = df['Summary'].apply(get_sentiment_score)
df['sentiment_category'] = df['sentiment_score'].apply(categorize_sentiment)

# Aggregate sentiment counts by category
sentiment_counts = df.groupby(['category', 'sentiment_category']).size().unstack(fill_value=0)

# Save per-review sentiment data
df.to_csv('flipkart_sentiment_per_review.csv', index=False)

# Save aggregated sentiment counts by category
sentiment_counts.to_csv('sentiment_counts_by_category.csv')

# Exclude 'Miscellaneous' category from sentiment_counts before plotting
sentiment_counts_filtered = sentiment_counts.drop('Miscellaneous', errors='ignore')

# Plot without Miscellaneous
sentiment_counts_filtered.plot(kind='bar', stacked=True, figsize=(12,7), colormap='RdYlGn')
plt.title('Sentiment Distribution by Category (excluding Miscellaneous)')
plt.xlabel('Category')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=45)
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()

sentiment_percentages = sentiment_counts_filtered.div(sentiment_counts_filtered.sum(axis=1), axis=0) * 100
sentiment_percentages.plot(kind='bar', stacked=True, figsize=(12,7), colormap='RdYlGn')
plt.title('Sentiment % Distribution by Category (excluding Miscellaneous)')
plt.ylabel('Percentage of Reviews')
plt.xticks(rotation=45)
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()

top_positive = sentiment_percentages['Positive'].sort_values(ascending=False).head(5)
top_negative = sentiment_percentages['Negative'].sort_values(ascending=False).head(5)

print("Top 5 Categories by Positive Sentiment %:\n", top_positive)
print("\nTop 5 Categories by Negative Sentiment %:\n", top_negative)

sentiment_percentages['sentiment_score'] = sentiment_percentages['Positive'] - sentiment_percentages['Negative']
top_scores = sentiment_percentages['sentiment_score'].sort_values(ascending=False)
print(top_scores)

import seaborn as sns

# Combine top positive and top negative for comparison
top_combined = pd.concat([
    top_positive.rename('Positive'),
    top_negative.rename('Negative')
], axis=1).fillna(0)

top_combined.plot(kind='bar', figsize=(10,6), color=['green', 'red'])
plt.title('Top 5 Categories by Positive and Negative Sentiment')
plt.ylabel('Sentiment Percentage')
plt.xlabel('Category')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

loved = sentiment_percentages[sentiment_percentages['sentiment_score'] > 70]
print("Highly Loved Categories:\n", loved)

needs_attention = sentiment_percentages.sort_values(by='sentiment_score').head(3)
print("Bottom 3 Categories by Sentiment Score:\n", needs_attention)

import matplotlib.pyplot as plt
# Combine and label
combined = pd.concat([needs_attention,loved])
combined = combined.sort_values(by='sentiment_score')

# Set custom color: red for attention, green for loved
colors = ['firebrick'] * len(needs_attention) + ['seagreen'] * len(loved)

# Plot
plt.figure(figsize=(10, 6))
bars = plt.barh(combined.index, combined['sentiment_score'], color=colors)
plt.axvline(x=0, color='gray', linewidth=0.8)
plt.title("Categories That Are Loved vs. Need Attention (Sentiment Score)")
plt.xlabel("Sentiment Score (Positive - Negative)")
plt.ylabel("Category")

# Add score labels
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2,
             f'{bar.get_width():.1f}', va='center',
             ha='left' if bar.get_width() > 0 else 'right')

plt.tight_layout()
plt.show()

"""# Keyword extraction"""

!pip install yake

import yake

# Initialize YAKE keyword extractor
kw_extractor = yake.KeywordExtractor(lan="en", n=1, top=5)  # n=1: single-word keywords, top=5 per summary

# Function to extract keywords from each summary
def extract_keywords(text):
    try:
        keywords = kw_extractor.extract_keywords(str(text))
        return [kw[0] for kw in keywords]  # just the keyword strings
    except:
        return []

# Apply to each summary
df['keywords'] = df['Summary'].apply(extract_keywords)

# Save updated DataFrame with keywords
df.to_csv('flipkart_sentiment_keywords.csv', index=False)

from collections import Counter
import ast

# Convert stringified lists back to actual Python lists if needed
df['keywords'] = df['keywords'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

# Create a dictionary to hold keyword frequencies by category
category_keywords = {}

for category in df['category'].unique():
    # Get all keywords in this category
    keywords_list = df[df['category'] == category]['keywords'].explode().dropna().tolist()

    # Count frequencies
    keyword_freq = Counter(keywords_list)

    # Store top 10 keywords (or more if you prefer)
    category_keywords[category] = keyword_freq.most_common(15)

# Convert to a DataFrame for easier viewing or export
category_keywords_df = pd.DataFrame.from_dict(category_keywords, orient='index')
category_keywords_df.columns = [f"Keyword {i+1}" for i in range(category_keywords_df.shape[1])]

# Save to CSV
category_keywords_df.to_csv("top_keywords_by_category.csv")

# Display
category_keywords_df.head()

!pip install wordcloud

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter
import ast

# Load the dataset
df = pd.read_csv('/content/flipkart_sentiment_keywords.csv')

# Convert stringified lists to actual lists (if necessary)
df['keywords'] = df['keywords'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

# Get unique categories as a list
category_list = df['category'].dropna().unique().tolist()

# Optional: skip small/noisy categories
skip_categories = ['Miscellaneous', 'Pooja Items']

def plot_wordcloud_for_category(category_name):
    # Get all keywords for the category
    keywords = df[df['category'] == category_name]['keywords'].explode().dropna().tolist()
    keyword_freq = Counter(keywords)

    # If no keywords, skip
    if not keyword_freq:
        print(f"Skipping {category_name}: no keywords found.")
        return

    # Generate word cloud
    wordcloud = WordCloud(
        width=800, height=400, background_color='white'
    ).generate_from_frequencies(keyword_freq)

    # Save word cloud image (optional)
    wordcloud.to_file(f"{category_name}_wordcloud.png")

    # Plot
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Top Keywords in {category_name}", fontsize=16)
    plt.tight_layout()
    plt.show()

# Generate word clouds for each category (excluding skipped ones)
for category in category_list:
    if category not in skip_categories:
        plot_wordcloud_for_category(category)